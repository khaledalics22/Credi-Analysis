{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f16ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import scipy.stats as stats\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import chi2_contingency\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import  sklearn.svm as svm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05ac26",
   "metadata": {},
   "source": [
    "# 1. Load Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950912ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f717d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_dataset = pd.read_csv('dataset/current_app.csv', na_values= [\"N/a\", \"na\", \"XNA\", np.nan])\n",
    "org_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b861f9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f3b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5d023",
   "metadata": {},
   "source": [
    "Notice: the data set has about 307k rows and 122 features (regressors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a186fccf",
   "metadata": {},
   "source": [
    "# 2. Cleaning Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca7e65",
   "metadata": {},
   "source": [
    "## 2.1 visulalize and analyze Nan Values in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8924c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clean_dataset = org_dataset.copy()\n",
    "to_clean_dataset.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05953f01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msno.matrix(to_clean_dataset,figsize=(20,10),color=(0, 0 ,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6077613",
   "metadata": {},
   "source": [
    "Notice: the dataset has many rows with a lot of Nan values \n",
    "recommended 1: to drop columns with nan values of percentage > 50% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce18373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clean_dataset.dropna(thresh= to_clean_dataset.shape[0]* 0.5, axis = 1, inplace=True )\n",
    "to_clean_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a02bd06",
   "metadata": {},
   "source": [
    "Notice: the number of columns decreased from 122 to 81 <br>\n",
    "when dropped columns with nan values of percentage > 50% <br> \n",
    "recommended 2: fill columns with Nan values < 13% with mean or mode depending on column type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561fba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_cols =to_clean_dataset.columns[to_clean_dataset.isnull().sum() >0.13 * to_clean_dataset.shape[0]]\n",
    "dirty_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf2ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clean_dataset[dirty_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d56230",
   "metadata": {},
   "source": [
    "Notice: We will replace with mode values for categorical columns and mean for continous columns<br> \n",
    "but for this we will first check if there are any outliers so that the mean values would be correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087be758",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clean_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8b916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dirty_cols2 = dirty_cols.delete([0,1,10])\n",
    "_ = to_clean_dataset[dirty_cols2].hist(figsize=(20,20) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c9b8eb",
   "metadata": {},
   "source": [
    "Notice: All columns seems to have valid values (ignoring Nan)<br> \n",
    "Now we can fill the columns with mean and mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121967ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clean_dataset['OCCUPATION_TYPE'].fillna(value= to_clean_dataset['OCCUPATION_TYPE'].mode()[0] ,inplace = True )\n",
    "to_clean_dataset['ORGANIZATION_TYPE'].fillna(value= to_clean_dataset['ORGANIZATION_TYPE'].mode()[0] ,inplace = True )\n",
    "to_clean_dataset['EMERGENCYSTATE_MODE'].fillna(value= to_clean_dataset['EMERGENCYSTATE_MODE'].mode()[0] ,inplace = True )\n",
    "to_clean_dataset['CODE_GENDER'].fillna(value= to_clean_dataset['CODE_GENDER'].mode()[0] ,inplace = True )\n",
    "\n",
    "to_clean_dataset['EXT_SOURCE_3'].fillna(value= to_clean_dataset['EXT_SOURCE_3'].mean() ,inplace = True )\n",
    "to_clean_dataset['YEARS_BEGINEXPLUATATION_AVG'].fillna(value= to_clean_dataset['YEARS_BEGINEXPLUATATION_AVG'].mean() ,inplace = True )\n",
    "to_clean_dataset['FLOORSMAX_AVG'].fillna(value= to_clean_dataset['FLOORSMAX_AVG'].mean() ,inplace = True )\n",
    "to_clean_dataset['YEARS_BEGINEXPLUATATION_MODE'].fillna(value= to_clean_dataset['YEARS_BEGINEXPLUATATION_MODE'].mean() ,inplace = True )\n",
    "to_clean_dataset['FLOORSMAX_MODE'].fillna(value= to_clean_dataset['FLOORSMAX_MODE'].mean() ,inplace = True )\n",
    "to_clean_dataset['YEARS_BEGINEXPLUATATION_MEDI'].fillna(value= to_clean_dataset['YEARS_BEGINEXPLUATATION_MEDI'].mean() ,inplace = True )\n",
    "to_clean_dataset['TOTALAREA_MODE'].fillna(value= to_clean_dataset['TOTALAREA_MODE'].mean() ,inplace = True )\n",
    "to_clean_dataset['AMT_REQ_CREDIT_BUREAU_HOUR'].fillna(value= to_clean_dataset['AMT_REQ_CREDIT_BUREAU_HOUR'].mean() ,inplace = True )\n",
    "to_clean_dataset['AMT_REQ_CREDIT_BUREAU_DAY'].fillna(value= to_clean_dataset['AMT_REQ_CREDIT_BUREAU_DAY'].mean() ,inplace = True )\n",
    "to_clean_dataset['AMT_REQ_CREDIT_BUREAU_WEEK'].fillna(value= to_clean_dataset['AMT_REQ_CREDIT_BUREAU_WEEK'].mean() ,inplace = True )\n",
    "to_clean_dataset['AMT_REQ_CREDIT_BUREAU_MON'].fillna(value= to_clean_dataset['AMT_REQ_CREDIT_BUREAU_MON'].mean() ,inplace = True )\n",
    "to_clean_dataset['AMT_REQ_CREDIT_BUREAU_QRT'].fillna(value= to_clean_dataset['AMT_REQ_CREDIT_BUREAU_QRT'].mean() ,inplace = True )\n",
    "to_clean_dataset['AMT_REQ_CREDIT_BUREAU_YEAR'].fillna(value= to_clean_dataset['AMT_REQ_CREDIT_BUREAU_YEAR'].mean() ,inplace = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86d41c5",
   "metadata": {},
   "source": [
    "Notice: For columns with Nan values > 13% and <50% we will drop rows containing Nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a427565",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "to_clean_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c7b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clean_dataset.dropna(inplace=True)\n",
    "to_clean_dataset.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cbf87f",
   "metadata": {},
   "source": [
    "Notice: Now our dataset has no Nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28deb944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove SK_ID_CURR Columns from dataset as it won't be useful for our problem \n",
    "to_clean_dataset = to_clean_dataset.drop(labels=['SK_ID_CURR'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b301bfec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_clean_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fdd59f",
   "metadata": {},
   "source": [
    "## 2.2 Check invalid values in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ed245",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clean_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ec1fa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = to_clean_dataset[to_clean_dataset.columns[:len(to_clean_dataset.columns)//2]].hist(figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b866987",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51525764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_clean_dataset_ = to_clean_dataset[to_clean_dataset.columns[len(to_clean_dataset.columns)//2:]].hist(figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05206c40",
   "metadata": {},
   "source": [
    "Notice: Some variables have negative values where they should be positive such as; <br> \n",
    "DAYS_BIRTH, DAYES_REGISTRATION, DAYES_ID_PUBLISH, DAYES_EMPLOYED, DAYES_LAST_PHONE_CHANGE <br> \n",
    "assuming the negative sign is added falsely we will convert values to positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1277a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clean_dataset['DAYS_BIRTH'] = abs(to_clean_dataset['DAYS_BIRTH'] )\n",
    "to_clean_dataset['DAYS_REGISTRATION'] = abs(to_clean_dataset['DAYS_REGISTRATION'] )\n",
    "to_clean_dataset['DAYS_ID_PUBLISH'] = abs(to_clean_dataset['DAYS_ID_PUBLISH'] )\n",
    "to_clean_dataset['DAYS_EMPLOYED'] = abs(to_clean_dataset['DAYS_EMPLOYED'] )\n",
    "to_clean_dataset['DAYS_LAST_PHONE_CHANGE'] = abs(to_clean_dataset['DAYS_LAST_PHONE_CHANGE'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4653b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_clean_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f87a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_features = [col for col in to_clean_dataset.columns if to_clean_dataset[col].unique().size <= 2]\n",
    "binary_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c341f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset = to_clean_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998414da",
   "metadata": {},
   "source": [
    "Notice: Considering the very large amount of features<br>  \n",
    "We will apply bivariate analysis first to remove redundant features<br> \n",
    "then We will analyize the usefull features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6941f9a9",
   "metadata": {},
   "source": [
    "# 3. Bivariate Analysis\n",
    "## 3.1 Continuous Vs Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d16498",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cols = clean_dataset.columns[(clean_dataset.dtypes == 'int64')|(clean_dataset.dtypes == 'float64')]\n",
    "cont_cols = [col for col in cont_cols if col not in binary_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733bcbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlated(cor, thr):\n",
    "    corr_idx = []\n",
    "    for i in range(cor.shape[0]): \n",
    "        for j in range(i, cor.shape[1]): \n",
    "            if i != j and abs(cor[i,j]) >= thr:\n",
    "                corr_idx.append((i,j))\n",
    "    \n",
    "    return corr_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35be8c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = clean_dataset[cont_cols].corr()\n",
    "plt.figure(figsize=(50,50))\n",
    "_ = sns.heatmap(data = cor ,cmap=plt.cm.CMRmap_r, annot= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edbbe53",
   "metadata": {},
   "source": [
    "Notice: From the heat map above we can see that some features are correlated.<br> \n",
    "So, we will remove features to get independent features. our correlation threshold is 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_idx = get_correlated(cor.to_numpy(), 0.85)\n",
    "indicies =np.unique([idx for idx,_ in corr_idx])\n",
    "for idx in indicies:\n",
    "    clean_dataset.drop(cont_cols[idx], axis = 1, inplace=True)\n",
    "clean_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16ca77f",
   "metadata": {},
   "source": [
    "Notice: the features decreased from 80 to 71 features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e45d25",
   "metadata": {},
   "source": [
    "## 3.2 Continous Vs Output (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31969be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cols = clean_dataset.columns[(clean_dataset.dtypes == 'int64')|(clean_dataset.dtypes == 'float64')]\n",
    "cont_cols = [col for col in cont_cols if col not in binary_features]\n",
    "\n",
    "cat_cols = clean_dataset.columns[(clean_dataset.dtypes != 'int64')&(clean_dataset.dtypes != 'float64')].values\n",
    "cat_cols = np.append(cat_cols, binary_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae27e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_cat_boxplot(cat_col, cont_col):\n",
    "    plt.figure()\n",
    "    sns.boxplot(data = clean_dataset, x= cat_col, y = cont_col, order=clean_dataset[cat_col].value_counts().index)\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "def cont_vs_out(out, cont_col):\n",
    "    plt.figure()\n",
    "    sns.boxplot(data = clean_dataset, x= out, y = cont_col, order=out.value_counts().index)\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f339646",
   "metadata": {},
   "source": [
    "Notice: We will remove features that are not useful for prediction<br> \n",
    "From the box plot, if Y axis has same distribution for all categories of categorical<br> \n",
    "feature on X axis, then there is no relation between Y axis and X axis, hence, we will remove this feature\n",
    "because it is a weak regressor. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58eb12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y = clean_dataset['TARGET']\n",
    "for j in range(len(cont_cols)):\n",
    "    print(cont_cols[j])\n",
    "    cont_vs_out(Y, cont_cols[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56789ad",
   "metadata": {},
   "source": [
    "Notice: After drawing box plot of continuous features vs output TARGET<br> \n",
    "We notice that the distribution for many features is almost the same for both output values<br> \n",
    "Which indicates that the output doesn't depend on those features. From the above plot we decided to remove the following features as they won't be useful: ['REGION_POPULATION_RELATIVE', 'DAYS_BIRTH','DAYS_REGISTRATION',\n",
    "                    'DAYS_ID_PUBLISH','REGION_RATING_CLIENT_W_CITY','HOUR_APPR_PROCESS_START'\n",
    "                    'LIVE_REGION_NOT_WORK_REGION','REG_CITY_NOT_LIVE_CITY','LIVE_CITY_NOT_WORK_CITY',\n",
    "                    'YEARS_BEGINEXPLUATATION_MEDI','FLOORSMAX_MEDI','TOTALAREA_MODE','DAYS_LAST_PHONE_CHANGE',\n",
    "                    'AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "                    'AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_YEAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc7dcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added 'DAYS_BIRTH', 'DAYS_ID_PUBLISH' , 'HOUR_APPR_PROCESS_START', 'FLOORSMAX_MEDI'\n",
    "useless_features = ['AMT_ANNUITY','AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE','DAYS_REGISTRATION',\n",
    "                    'REGION_RATING_CLIENT_W_CITY',\n",
    "                    'YEARS_BEGINEXPLUATATION_MEDI','TOTALAREA_MODE','DAYS_LAST_PHONE_CHANGE',\n",
    "                    'AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "                    'AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_YEAR']\n",
    "len(useless_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fabab9",
   "metadata": {},
   "source": [
    "Notice: Columns; OBS_60_CNT_SOCIAL_CIRCLE, DEF_60_CNT_SOCIAL_CIRCLE, AMT_REQ_CREDIT_BUREAU_QRT, AMT_INCOME_TOTAL, DAYS_EMPLOYED have outlier we will remove the ouliers and see distribution again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset.drop((clean_dataset[clean_dataset.OBS_60_CNT_SOCIAL_CIRCLE > 40].index), axis = 0, inplace= True)\n",
    "clean_dataset.drop((clean_dataset[clean_dataset.DEF_60_CNT_SOCIAL_CIRCLE > 20].index), axis = 0, inplace= True)\n",
    "clean_dataset.drop((clean_dataset[clean_dataset.AMT_REQ_CREDIT_BUREAU_QRT > 200].index), axis = 0, inplace= True)\n",
    "clean_dataset.drop((clean_dataset[clean_dataset.AMT_INCOME_TOTAL > 0.4e6].index), axis = 0, inplace= True)\n",
    "clean_dataset.drop((clean_dataset[clean_dataset.DAYS_EMPLOYED > 300000].index), axis = 0, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29696ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cont_vs_out(Y, clean_dataset.OBS_60_CNT_SOCIAL_CIRCLE)\n",
    "cont_vs_out(Y, clean_dataset.DEF_60_CNT_SOCIAL_CIRCLE)\n",
    "cont_vs_out(Y, clean_dataset.AMT_REQ_CREDIT_BUREAU_QRT)\n",
    "cont_vs_out(Y, clean_dataset.AMT_INCOME_TOTAL)\n",
    "cont_vs_out(Y, clean_dataset.DAYS_EMPLOYED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e6325",
   "metadata": {},
   "source": [
    "Notice: They also are independent of output so we will remove them too except DAYS_EMPLOYES, AMT_INCOME_TOTAL there is sort of dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d62aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_features.extend(['OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE', 'AMT_REQ_CREDIT_BUREAU_QRT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21334c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset.drop(useless_features, axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a5c7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7171e",
   "metadata": {},
   "source": [
    "Notice Now we have 51 regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb640c8",
   "metadata": {},
   "source": [
    "## 3.3 Binary Categorical Vs Output (binary categorical) \n",
    "### For this problem we will use pearson r method to calculate correlation between \n",
    "### binary categorical and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361a42fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert binary categorical to numerical values\n",
    "clean_dataset['FLAG_OWN_REALTY'] = pd.get_dummies(clean_dataset['FLAG_OWN_REALTY'], drop_first=True, prefix = 'FLAG_OWN_REALTY')\n",
    "clean_dataset['FLAG_OWN_CAR'] = pd.get_dummies(clean_dataset['FLAG_OWN_CAR'], drop_first=True, prefix = 'FLAG_OWN_CAR')\n",
    "clean_dataset['CODE_GENDER'] = pd.get_dummies(clean_dataset['CODE_GENDER'], drop_first=True, prefix = 'CODE_GENDER')\n",
    "clean_dataset['NAME_CONTRACT_TYPE'] = pd.get_dummies(clean_dataset['NAME_CONTRACT_TYPE'], drop_first=True, prefix = 'NAME_CONTRACT_TYPE')\n",
    "clean_dataset['EMERGENCYSTATE_MODE'] = pd.get_dummies(clean_dataset['EMERGENCYSTATE_MODE'], drop_first=True, prefix = 'EMERGENCYSTATE_MODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85733fed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(binary_features)):\n",
    "    r = stats.pearsonr(clean_dataset[binary_features[i]], clean_dataset['TARGET'])\n",
    "    print(f'corr of {binary_features[i]} vs TARGET = {r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbff8dc",
   "metadata": {},
   "source": [
    "Notice: All binary features except CODE_GENDER, NAME_CONTRACT_TYPE, FLAG_OWN_CAR,FLAG_PHONE, \n",
    "REG_CITY_NOT_LIVE_CITY,REG_CITY_NOT_WORK_CITY,FLAG_DOCUMENT_3 doesn't have any correlation with output, so we will drop all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd34fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_to_keep = ['CODE_GENDER', 'NAME_CONTRACT_TYPE', 'FLAG_OWN_CAR','FLAG_PHONE', \n",
    "'REG_CITY_NOT_LIVE_CITY','REG_CITY_NOT_WORK_CITY','FLAG_DOCUMENT_3','TARGET']\n",
    "features = [f for f in binary_features if f not in binary_to_keep]\n",
    "clean_dataset.drop(features, axis = 1, inplace=True)\n",
    "clean_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08805ea6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clean_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d7c5be",
   "metadata": {},
   "source": [
    "## 3.4 Multiple Categorical Vs Continuous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b75f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = clean_dataset.columns[(clean_dataset.dtypes == 'object')]\n",
    "cont_cols = clean_dataset.columns[(clean_dataset.dtypes == 'int64')|(clean_dataset.dtypes == 'float64')]\n",
    "cont_cols = [col for col in cont_cols if col not in binary_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ddf56d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range(len(cont_cols)):\n",
    "    for i in range(len(cat_cols)):\n",
    "        print(f'{cont_cols[j]} vs {cat_cols[i]}')\n",
    "        cont_cat_boxplot(cat_cols[i], cont_cols[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb3060",
   "metadata": {},
   "source": [
    "Notice: For this analysis we will remove the redundant feature that is correlated to another existing one <br> \n",
    "because we need all features to be indepenedent <br> \n",
    "We can notice that AMT_INCOME_TOTAL vs NAME_INCOME_TYPE, AMT_INCOME_TOTAL vs NAME_EDUCATION_TYPE ,\n",
    " CNT_FAM_MEMBERS vs NAME_FAMILY_STATUS are dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5be091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added NAME_EDUCATION_TYPE\n",
    "clean_dataset.drop(['NAME_INCOME_TYPE',], axis = 1, inplace= True)\n",
    "clean_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455d6cd8",
   "metadata": {},
   "source": [
    "## 3.5 Multiple Categorical Vs Categorical\n",
    "For this analysis we will use chi2 test to calculate the correlation between the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f6ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = clean_dataset.columns[(clean_dataset.dtypes == 'object')]\n",
    "cat_cols =np.append(cat_cols.values,binary_to_keep)\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_encoded = clean_dataset.copy()\n",
    "label = preprocessing.LabelEncoder()\n",
    "encoded_data = pd.DataFrame(to_encoded)\n",
    "for col in cat_cols:\n",
    "    encoded_data[col] = label.fit_transform(to_encoded[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cramerV(col1, col2):\n",
    "    cross_table = pd.crosstab(col1, col2)\n",
    "    chi2 = chi2_contingency(cross_table)[0]\n",
    "    n = cross_table.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = cross_table.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))\n",
    "\n",
    "rows = []\n",
    "for i in range(len(cat_cols)):\n",
    "    cols = []\n",
    "    for j in range(len(cat_cols)):\n",
    "        V = calc_cramerV(encoded_data[cat_cols[i]],encoded_data[cat_cols[j]])\n",
    "        cols.append(V)\n",
    "    rows.append(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c3639",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = pd.DataFrame(data = rows, index= cat_cols,columns = cat_cols )\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b99154f",
   "metadata": {},
   "source": [
    "From the above matrix the correlation between we will remove a feature if correlated with another for over 85% <br> \n",
    "and we will keep a features if correlated with target output for over 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb81aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlated with target \n",
    "f_sel_data = clean_dataset.copy()\n",
    "f_sel_data.drop(['NAME_TYPE_SUITE', 'NAME_HOUSING_TYPE', 'WEEKDAY_APPR_PROCESS_START', \n",
    "                    'FLAG_OWN_CAR', 'FLAG_PHONE', 'REG_CITY_NOT_LIVE_CITY','REG_CITY_NOT_WORK_CITY'], axis = 1, inplace= True)\n",
    "# total_features = ['OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'CODE_GENDER','DAYS_EMPLOYED','AMT_INCOME_TOTAL',\n",
    "#                  'CNT_FAM_MEMBERS','EXT_SOURCE_2','EXT_SOURCE_3','TARGET']\n",
    "# # ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77507a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_sel_data.head()\n",
    "# f_sel_data = f_sel_data[total_features]\n",
    "# f_sel_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c011ed",
   "metadata": {},
   "source": [
    "# Univariate Analaysis (Understand the Variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0b8df",
   "metadata": {},
   "source": [
    "### Distribution of applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea728551",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [ 'Non-Defaulters', 'Defaulters']\n",
    "plt.title(\"Distribution of Apps\")\n",
    "_ = plt.pie(f_sel_data.TARGET.value_counts(),labels=labels, explode= (0.1,0.1), autopct= \"%1.1f%%\", startangle=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7087bf51",
   "metadata": {},
   "source": [
    "Notice: that our dataset has 93% Non-Defaulters and 7% Defaulters so our dataset is <b>Impalanced<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5cb84",
   "metadata": {},
   "source": [
    "### Female Vs Male applicants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19560c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(4)\n",
    "_ = sns.countplot(x = 'TARGET', data= f_sel_data[f_sel_data.CODE_GENDER == 0], ax= ax1)\n",
    "ax1.set_title('Distribution of Females')\n",
    "ax1.set_ylim(0, len(f_sel_data))\n",
    "ax1.set_xticklabels(['Non-Defaulter', 'Defaulter'])\n",
    "\n",
    "_ = sns.countplot(x = 'TARGET', data= f_sel_data[f_sel_data.CODE_GENDER == 1], ax= ax2)\n",
    "ax2.set_title('Distribution of Males')\n",
    "ax2.set_ylim(0, len(f_sel_data))\n",
    "ax2.set_xticklabels(['Non-Defaulter', 'Defaulter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b0b96",
   "metadata": {},
   "source": [
    "Notice: Here the number of Non-defalters for men is less than the number of defaulters for women\n",
    "To get better insight we better calculate percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b12d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(4)\n",
    "labels = [ 'Non-Defaulters', 'Defaulters']\n",
    "ax1.pie(f_sel_data[f_sel_data.CODE_GENDER == 1].TARGET.value_counts(),labels=labels, explode= (0.1,0.1), autopct= \"%1.1f%%\", startangle=90)\n",
    "ax1.set_title('Percentage of Males')\n",
    "\n",
    "ax2.pie(f_sel_data[f_sel_data.CODE_GENDER == 0].TARGET.value_counts(),labels=labels, explode= (0.1,0.1), autopct= \"%1.1f%%\", startangle=90)\n",
    "ax2.set_title('Percentage of Females')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca94e8a",
   "metadata": {},
   "source": [
    "Notice: Now we can see that the percentage of Non-Defaulted females is larger than men "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eae962",
   "metadata": {},
   "source": [
    "###  Family members for females and males "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25f00fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.xticks(rotation = 45)\n",
    "plt.title(\"Occupation type for Males\")\n",
    "sns.countplot(x = 'OCCUPATION_TYPE', data= f_sel_data[f_sel_data.CODE_GENDER == 1])\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.xticks(rotation = 45)\n",
    "plt.title(\"Occupation type for Females\")\n",
    "sns.countplot(x = 'OCCUPATION_TYPE', data= f_sel_data[f_sel_data.CODE_GENDER == 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e2c5de",
   "metadata": {},
   "source": [
    "Notice: we can see that most people who want a credit are laborers<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba2531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = f_sel_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv(\"Selected_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3899b931",
   "metadata": {},
   "source": [
    "# Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a104e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import scipy.stats as stats\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import chi2_contingency\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import  sklearn.svm as svm\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74bf333",
   "metadata": {},
   "source": [
    "### Encoding Categorial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3145ae86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>NAME_EDUCATION_TYPE</th>\n",
       "      <th>NAME_FAMILY_STATUS</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>DAYS_ID_PUBLISH</th>\n",
       "      <th>OCCUPATION_TYPE</th>\n",
       "      <th>CNT_FAM_MEMBERS</th>\n",
       "      <th>HOUR_APPR_PROCESS_START</th>\n",
       "      <th>ORGANIZATION_TYPE</th>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "      <th>FLOORSMAX_MEDI</th>\n",
       "      <th>FLAG_DOCUMENT_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>9461</td>\n",
       "      <td>637</td>\n",
       "      <td>2120</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>0.262949</td>\n",
       "      <td>0.139376</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Married</td>\n",
       "      <td>16765</td>\n",
       "      <td>1188</td>\n",
       "      <td>291</td>\n",
       "      <td>Core staff</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11</td>\n",
       "      <td>School</td>\n",
       "      <td>0.622246</td>\n",
       "      <td>0.510853</td>\n",
       "      <td>0.2917</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>13439</td>\n",
       "      <td>2717</td>\n",
       "      <td>3227</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Business Entity Type 2</td>\n",
       "      <td>0.715042</td>\n",
       "      <td>0.176653</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>14086</td>\n",
       "      <td>3028</td>\n",
       "      <td>4911</td>\n",
       "      <td>Drivers</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>0.566907</td>\n",
       "      <td>0.770087</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>189000.0</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>14583</td>\n",
       "      <td>203</td>\n",
       "      <td>2056</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>Transport: type 2</td>\n",
       "      <td>0.642656</td>\n",
       "      <td>0.510853</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TARGET  NAME_CONTRACT_TYPE  CODE_GENDER  AMT_INCOME_TOTAL  \\\n",
       "0       1                   0            1          202500.0   \n",
       "1       0                   0            0          270000.0   \n",
       "2       0                   0            0           67500.0   \n",
       "3       0                   0            1          225000.0   \n",
       "4       0                   0            0          189000.0   \n",
       "\n",
       "             NAME_EDUCATION_TYPE    NAME_FAMILY_STATUS  DAYS_BIRTH  \\\n",
       "0  Secondary / secondary special  Single / not married        9461   \n",
       "1               Higher education               Married       16765   \n",
       "2  Secondary / secondary special               Married       13439   \n",
       "3  Secondary / secondary special               Married       14086   \n",
       "4  Secondary / secondary special               Married       14583   \n",
       "\n",
       "   DAYS_EMPLOYED  DAYS_ID_PUBLISH OCCUPATION_TYPE  CNT_FAM_MEMBERS  \\\n",
       "0            637             2120        Laborers              1.0   \n",
       "1           1188              291      Core staff              2.0   \n",
       "2           2717             3227        Laborers              2.0   \n",
       "3           3028             4911         Drivers              3.0   \n",
       "4            203             2056        Laborers              2.0   \n",
       "\n",
       "   HOUR_APPR_PROCESS_START       ORGANIZATION_TYPE  EXT_SOURCE_2  \\\n",
       "0                       10  Business Entity Type 3      0.262949   \n",
       "1                       11                  School      0.622246   \n",
       "2                       10  Business Entity Type 2      0.715042   \n",
       "3                       13           Self-employed      0.566907   \n",
       "4                        9       Transport: type 2      0.642656   \n",
       "\n",
       "   EXT_SOURCE_3  FLOORSMAX_MEDI  FLAG_DOCUMENT_3  \n",
       "0      0.139376          0.0833                1  \n",
       "1      0.510853          0.2917                1  \n",
       "2      0.176653          0.1667                1  \n",
       "3      0.770087          0.3333                1  \n",
       "4      0.510853          0.6667                1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv(\"Selected_dataset.csv\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8395397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(X):\n",
    "    Y = X.TARGET\n",
    "    X.drop(['TARGET'] , axis = 1, inplace = True)\n",
    "    num_cols = ['DAYS_BIRTH','FLOORSMAX_MEDI','HOUR_APPR_PROCESS_START','DAYS_ID_PUBLISH','CNT_FAM_MEMBERS','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_EMPLOYED','AMT_INCOME_TOTAL']\n",
    "    mu = X[num_cols].mean(axis = 0)\n",
    "    X[num_cols] = X[num_cols] - mu\n",
    "    X[num_cols] /= X[num_cols].max(axis = 0)\n",
    "    encoder = OneHotEncoder(drop='if_binary', sparse=True)\n",
    "    cat_cols = X.columns[X.dtypes == 'object']\n",
    "    encoded_cols = encoder.fit_transform(X[cat_cols])\n",
    "    encoded_X = X.drop(cat_cols, axis = 1)\n",
    "    encoded_X = np.append( encoded_cols.toarray(), encoded_X.to_numpy(), axis= 1)\n",
    "    return encoded_X, Y.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57305496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize continous data\n",
    "encoded_X, Y = encode_data(X)\n",
    "\n",
    "total = np.arange(0,len(X), 1)\n",
    "test_indx = np.random.randint(0,len(X),size = int(0.1* len(X)))\n",
    "train_indx = np.setdiff1d(total, test_indx)\n",
    "\n",
    "X_test = encoded_X[test_indx].copy()\n",
    "y_test = Y[test_indx].copy()\n",
    "Y = Y[train_indx].copy()\n",
    "encoded_X = encoded_X[train_indx].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7d818a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109596, 97)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f91e3ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12109, 97)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b1d3d",
   "metadata": {},
   "source": [
    "### Helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2178410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_clf(X_train, X_valid, y_train, y_valid,kernel , degree, nu = None,c = 1.0, plot = True):\n",
    "    #fit model\n",
    "    clf = None \n",
    "    if nu == None: \n",
    "        clf = svm.SVC(kernel = kernel, gamma='auto',C = c, degree = degree)\n",
    "    else: \n",
    "        clf = svm.Nu_SVC(gamma='auto')\n",
    "        \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    #predict train set\n",
    "    train_pred = clf.predict(X_train)\n",
    "    valid_pred = clf.predict(X_valid)\n",
    "    if plot: \n",
    "        print(f'Training Accuracy: {(train_pred == y_train).mean()*100:.2f} , Test Accuracy: {(valid_pred == y_valid).mean()*100:.2f}')\n",
    "        plot_confusion(y_train, train_pred, y_valid, valid_pred)\n",
    "    return clf, (train_pred == y_train).mean(),(valid_pred == y_valid).mean()*100\n",
    "\n",
    "def plot_confusion(y_train, train_pred, y_valid, valid_pred):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(8)\n",
    "    \n",
    "    ax1.set_title(\"Training Confuction Matrix\")\n",
    "    ax2.set_title(\"Testing Confuction Matrix\")\n",
    "    plt.subplots_adjust(left=0.1, bottom=0.1,right=0.9, top=0.9, wspace=0.5, hspace=0.4)\n",
    "    \n",
    "    cm = confusion_matrix(y_train, train_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm)\n",
    "    disp.plot(ax = ax1)\n",
    "     \n",
    "    # predict test set \n",
    "   \n",
    "    cm = confusion_matrix(y_valid, valid_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm)\n",
    "    disp.plot(ax = ax2)\n",
    "    \n",
    "def over_sample(X, Y, test_size = 0.2):\n",
    "    ros = RandomOverSampler()\n",
    "    return ros.fit_resample(X, Y)\n",
    "    \n",
    "    \n",
    "def under_sample(X,Y, test_size = 0.2): \n",
    "    rus = RandomUnderSampler()\n",
    "    return rus.fit_resample(X,Y)\n",
    "    \n",
    "\n",
    "def dt_clf(X_train, X_valid, y_train, y_valid,max_depth = 34, plot = True):\n",
    "    dt = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    dt.fit(X_train, y_train)\n",
    "    train_pred = dt.predict(X_train)\n",
    "    valid_pred = dt.predict(X_valid)\n",
    "    if plot: \n",
    "        print(f\"Max-Depth is: {dt.tree_.max_depth}\")\n",
    "        print(f\"DT: Train Acc: {(train_pred == y_train).mean()*100:.2f} , Test Acc: {(valid_pred == y_valid).mean()*100:.2f}\")\n",
    "        plot_confusion(y_train, train_pred, y_test, test_pred)\n",
    "    return dt, (train_pred == y_train).mean()*100,(valid_pred == y_valid).mean()*100\n",
    "\n",
    "def knn_clf(X_train, X_valid, y_train, y_valid,k = 5 ,plot = True):\n",
    "    clf = KNeighborsClassifier(n_neighbors = k)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_pred = clf.predict(X_train)\n",
    "    valid_pred = clf.predict(X_valid)\n",
    "    if plot:\n",
    "        print(f'{int(np.sqrt(len(X_train)))}-NN Clf -> Train Accuracy: {(train_pred == y_train).mean()*100:.2f},  Test Accuracy: {(valid_pred == y_valid).mean()*100:.2f}')\n",
    "        plot_confusion(y_train, train_pred, y_valid, valid_pred)\n",
    "    return clf, (train_pred == y_train).mean()*100,(valid_pred == y_valid).mean()*100\n",
    "\n",
    "def RF_clf(X_train, X_valid, y_train, y_valid, n_esimators,plot = True):\n",
    "    rf = RandomForestClassifier(n_estimators= n_esimators)\n",
    "    rf.fit(X_train, y_train)\n",
    "    train_pred = rf.predict(X_train)\n",
    "    valid_pred = rf.predict(X_valid)\n",
    "    if plot: \n",
    "        print(f\"DT: Train Acc: {(train_pred == y_train).mean()*100:.2f} , Test Acc: {(valid_pred == y_valid).mean()*100:.2f}\")\n",
    "        plot_confusion(y_train, train_pred, y_valid,  valid_pred)\n",
    "    return rf, (train_pred == y_train).mean()*100,(valid_pred == y_valid).mean()*100\n",
    "\n",
    "\n",
    "def tune_svm(X_train, X_valid, y_train, y_valid):\n",
    "    kernel = ['poly', 'rbf', 'linear','precomputed', 'sigmoid']\n",
    "    degree = np.arange(1, 10, 1)\n",
    "    C = np.arange(1, 100, 5)\n",
    "    best_model = (None, None, None)\n",
    "    max_acc = 0\n",
    "    for kr in kernel:\n",
    "        for deg in degree:\n",
    "            for c in C:\n",
    "                _, tacc, vacc = svm_clf(X_train, X_valid, y_train, y_valid, kernel = kr,\n",
    "                                        degree = deg,c = c, plot = False)\n",
    "                if vacc > max_acc:\n",
    "                    max_acc = vacc\n",
    "                    best_model = (kr, deg, c)\n",
    "                if tacc == 100:\n",
    "                    break\n",
    "    return best_model\n",
    "\n",
    "def tune_knn( X_train, X_valid, y_train, y_valid):\n",
    "    train_acc = [];  valid_acc = []\n",
    "    best_k = None\n",
    "    max_vacc = 0\n",
    "    for k in np.arange(5, int(np.sqrt(len(X_train))), 2):\n",
    "        _, tracc, vacc  = knn_clf(X_train, X_valid, y_train, y_valid,k , plot = False)\n",
    "        train_acc.extend([tracc])\n",
    "        valid_acc.extend([vacc])\n",
    "        if vacc > max_vacc:\n",
    "            max_vacc = vacc\n",
    "            best_k = k\n",
    "        if tracc == 100:\n",
    "            break\n",
    "            \n",
    "    plt.figure()\n",
    "    plt.title(\"Accuracy Graph vs K value\")\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.plot(np.arange(5, int(np.sqrt(len(X_train))), 2), train_acc)\n",
    "    plt.plot(np.arange(5, int(np.sqrt(len(X_train))), 2), valid_acc)\n",
    "    return best_k\n",
    "\n",
    "def tune_DT(X_train, X_valid, y_train, y_valid):\n",
    "    train_acc = [];  valid_acc = []\n",
    "    best_depth = None\n",
    "    max_vacc = 0\n",
    "    for i in np.arange(3,34,1):\n",
    "        _ , tracc, vacc = dt_clf(X_train, X_valid, y_train, y_valid, max_depth = i, plot= False)\n",
    "        train_acc.extend([tracc])\n",
    "        valid_acc.extend([vacc])\n",
    "        if vacc > max_vacc:\n",
    "            max_vacc = vacc\n",
    "            best_depth = i\n",
    "        if tracc == 100:\n",
    "            break\n",
    "    plt.figure()\n",
    "    plt.title(\"Accuracy Graph vs Max-Depth\")\n",
    "    plt.xlabel(\"Max-Depth\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.plot(np.arange(3,34,1), train_acc)\n",
    "    plt.plot(np.arange(3,34,1), valid_acc)\n",
    "    return best_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d40dec",
   "metadata": {},
   "source": [
    "## 1- Impalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa5612ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(encoded_X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecbb80f",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb33e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = tune_svm(X_train, X_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23565b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_model, _, _ = svm_clf(X_train, X_valid, y_train, y_valid,\n",
    "#                         kernel = best_model[0], degree =  best_model[1],c =  best_model[2], plot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62130be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = svm_model.predict(X_test)\n",
    "# plot_confusion(pred,y_test,pred,y_test)\n",
    "# print(f'Test Accuracy:{(pred == y_test).mean()}')\n",
    "# print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2a3ea4",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e2569a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = tune_knn(X_train, X_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfcbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model, _, _  = knn_clf(X_train, X_valid, y_train, y_valid, best_k, plot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad38db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = knn_model.predict(X_test)\n",
    "plot_confusion(pred,y_test,pred,y_test)\n",
    "print(f'Test Accuracy:{(pred == y_test).mean()}')\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d7dbc2",
   "metadata": {},
   "source": [
    "### DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d2554e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_depth = tune_DT(X_train, X_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a989a422",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model ,_,_ = dt_clf(X_train, X_valid, y_train, y_valid, max_depth = best_depth, plot= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89caf793",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dt_model.predict(X_test)\n",
    "plot_confusion(pred,y_test,pred,y_test)\n",
    "print(f'Test Accuracy:{(pred == y_test).mean()}')\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50331e72",
   "metadata": {},
   "source": [
    "### Random Forest with K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca318f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "# mean_train_acc = []\n",
    "# mean_valid_acc = []\n",
    "# best_fold = (None, None)\n",
    "# best_valid_acc = 0\n",
    "# train_acc = 0\n",
    "# for train, valid in kf.split(encoded_X,Y):\n",
    "#         _, tacc, vacc = RF_clf(encoded_X[train], encoded_X[valid], Y[train], Y[valid], n_esimators = 2, plot= False)\n",
    "#         mean_train_acc.extend([tacc])\n",
    "#         mean_valid_acc.extend([vacc])\n",
    "#         if vacc > best_valid_acc:\n",
    "#             best_valid_acc = vacc\n",
    "#             train_acc = tacc\n",
    "#             best_fold = (train, valid)\n",
    "        \n",
    "# plt.plot(range(len(mean_train_acc)),mean_train_acc)\n",
    "# plt.plot(range(len(mean_valid_acc)),mean_valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e81aef",
   "metadata": {},
   "source": [
    "### Final Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7131a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_valid, y_train, y_valid = encoded_X[best_fold[0]],encoded_X[best_fold[1]], Y[best_fold[0]], Y[best_fold[1]]\n",
    "# rf_model, vacc, vacc = RF_clf(X_train, X_valid, y_train, y_valid, n_esimators = 2, plot= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55005301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X_test,y_test = encode_data(test_data)\n",
    "# pred = rf_model.predict(X_test)\n",
    "# plot_confusion(pred,y_test,pred,y_test)\n",
    "# print(f'Test Accuracy:{(pred == y_test).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54394cf4",
   "metadata": {},
   "source": [
    "## 2- UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8365069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_us, Y_us = under_sample(encoded_X, Y, test_size=0.1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_us, Y_us, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaa82ec",
   "metadata": {},
   "source": [
    "### SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ccf99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = tune_svm(X_train, X_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18431cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_model, _, _ = svm_clf(X_train, X_valid, y_train, y_valid,\n",
    "#                         kernel = best_model[0], degree =  best_model[1],c =  best_model[2], plot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b86ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = svm_model.predict(X_test)\n",
    "# plot_confusion(pred,y_test,pred,y_test)\n",
    "# print(f'Test Accuracy:{(pred == y_test).mean()}')\n",
    "# print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03894bdc",
   "metadata": {},
   "source": [
    "### KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d15e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = tune_knn(X_train, X_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a303dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model, _, _  = knn_clf(X_train, X_valid, y_train, y_valid, best_k, plot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec43b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = knn_model.predict(X_test)\n",
    "plot_confusion(pred,y_test,pred,y_test)\n",
    "print(f'Test Accuracy:{(pred == y_test).mean()}')\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff5e672",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cab2d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_depth = tune_DT(X_train, X_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da43bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model ,_,_ = dt_clf(X_train, X_valid, y_train, y_valid, max_depth = best_depth, plot= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c837c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dt_model.predict(X_test)\n",
    "plot_confusion(pred,y_test,pred,y_test)\n",
    "print(f'Test Accuracy:{(pred == y_test).mean()}')\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c21e421",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa061cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# under sample dataset \n",
    "# rus = RandomUnderSampler()\n",
    "# X_res, y_res = rus.fit_resample(encoded_X,Y)\n",
    "# # K-Fold split\n",
    "# kf = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "# mean_train_acc = []\n",
    "# mean_valid_acc = []\n",
    "# best_fold = (None, None)\n",
    "# best_valid_acc = 0\n",
    "# train_acc = 0\n",
    "# for n in np.arange(2,100,5):\n",
    "#     mean_valid_acc = []\n",
    "#     mean_train_acc = []\n",
    "#     for train, valid in kf.split(X_res,y_res):\n",
    "#             _, tacc, vacc = RF_clf(X_res[train], X_res[valid], y_res[train], y_res[valid], n_esimators = n, plot= False)\n",
    "#             mean_train_acc.extend([tacc])\n",
    "#             mean_valid_acc.extend([vacc])\n",
    "#             if vacc > best_valid_acc:\n",
    "#                 best_valid_acc = vacc\n",
    "#                 train_acc = tacc\n",
    "#                 best_fold = (train, valid)\n",
    "\n",
    "#     plt.plot(range(len(mean_train_acc)),mean_train_acc)\n",
    "#     plt.plot(range(len(mean_valid_acc)),mean_valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_valid, y_train, y_valid = X_res[best_fold[0]],X_res[best_fold[1]], y_res[best_fold[0]], y_res[best_fold[1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19f63a",
   "metadata": {},
   "source": [
    "### OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4cb9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_os, Y_os = over_sample(encoded_X, Y, test_size=0.1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_os, Y_os, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc644265",
   "metadata": {},
   "source": [
    "### DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1861c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_depth = tune_DT(X_train, X_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0a6167",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model ,_,_ = dt_clf(X_train, X_valid, y_train, y_valid, max_depth = best_depth, plot= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e859f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dt_model.predict(X_test)\n",
    "plot_confusion(pred,y_test,pred,y_test)\n",
    "print(f'Test Accuracy:{(pred == y_test).mean()}')\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8608bee7",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a62bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = tune_knn(X_train, X_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a2c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model, _, _  = knn_clf(X_train, X_valid, y_train, y_valid, best_k, plot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a30c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = knn_model.predict(X_test)\n",
    "plot_confusion(pred,y_test,pred,y_test)\n",
    "print(f'Test Accuracy:{(pred == y_test).mean()}')\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba726579",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895bf70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = tune_svm(X_train, X_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a3ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_model, _, _ = svm_clf(X_train, X_valid, y_train, y_valid,\n",
    "#                         kernel = best_model[0], degree =  best_model[1],c =  best_model[2], plot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeacd337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = svm_model.predict(X_test)\n",
    "# plot_confusion(pred,y_test,pred,y_test)\n",
    "# print(f'Test Accuracy:{(pred == y_test).mean()}')\n",
    "# print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a10b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8e689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb32730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28431714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f52ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_valid, y_train, y_valid = over_sample(encoded_X, Y, test_size=0.1)\n",
    "# list_train_acc = []\n",
    "# list_valid_acc = []\n",
    "# best_valid_acc = 0\n",
    "# best_n = None\n",
    "# for n in np.arange(2,100,5):\n",
    "#     _, tacc, vacc = RF_clf(X_train, X_valid, y_train, y_valid, n_esimators = n, plot= False)\n",
    "#     list_train_acc.extend([tacc])\n",
    "#     list_valid_acc.extend([vacc])\n",
    "#     if tacc == 100:\n",
    "#         break\n",
    "#     if vacc > best_valid_acc:\n",
    "#         best_valid_acc = vacc\n",
    "#         best_n = n\n",
    "        \n",
    "# plt.figure()\n",
    "# plt.title(\"Accuracy Vs N_Esimators\")\n",
    "# plt.plot(range(len(list_train_acc)),list_train_acc)\n",
    "# plt.plot(range(len(list_valid_acc)),list_valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5128611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_model, _, _ = RF_clf(X_train, X_valid, y_train, y_valid, n_esimators = best_n, plot= False)\n",
    "# # print(f\"Traing Accuracy: {tracc}, Valid Accuracy: {vacc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X_test,y_test = encode_data(test_data)\n",
    "# pred = rf_model.predict(X_test)\n",
    "# plot_confusion(pred,y_test,pred,y_test)\n",
    "# print(f'Test Accuracy:{(pred == y_test).mean()}')\n",
    "# print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581ecff1",
   "metadata": {},
   "source": [
    "The Best Model Gotten is Random Forest With OverSampling "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
